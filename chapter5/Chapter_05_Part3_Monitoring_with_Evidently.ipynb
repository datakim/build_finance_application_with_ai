{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e6a088-8317-4484-bb1d-0e3c89134636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (100000, 919)\n",
      "Dropped 106 columns (>80.0% missing).\n",
      "After dropping high-missing columns: (100000, 813)\n",
      "Finished BFSI cleaning. Ready to define subset & do train/test.\n",
      "Current shape: (100000, 813)\n",
      "OptBinning said these columns are top: ['D_112_min', 'S_6_mean', 'R_15_mean', 'D_56_max', 'D_47_mean', 'B_6_std', 'D_51_min', 'D_79_last', 'B_5_max', 'D_65_std', 'D_81_std', 'D_81_max', 'D_65_mean', 'R_15_std', 'D_127_max', 'S_5_std', 'R_24_std', 'B_12_max', 'D_120_last']\n",
      "Subsetting to OptBinning columns. New shape: (100000, 20)\n",
      "Train shape: (70000, 19), Test shape: (30000, 19)\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# 1) Load and Clean BFSI Data from Scratch\n",
    "##############################################\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"overflow encountered\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"divide by zero encountered\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"The default of observed=False is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in reduce\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Example: same local pickle from sections 5.3 & 5.4\n",
    "df = pd.read_pickle(\"train_df_sample.pkl\")\n",
    "print(\"Initial shape:\", df.shape)\n",
    "\n",
    "def drop_null_cols(df, threshold=0.8):\n",
    "    null_percent = df.isnull().mean()\n",
    "    drop_cols = list(null_percent[null_percent > threshold].index)\n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "    print(f\"Dropped {len(drop_cols)} columns (>{threshold*100}% missing).\")\n",
    "    return df\n",
    "\n",
    "df = drop_null_cols(df, 0.8)\n",
    "print(\"After dropping high-missing columns:\", df.shape)\n",
    "\n",
    "# Suppose we label-encode the same known categorical columns from section 5.3\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_68\"]\n",
    "cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
    "cat_features = [c for c in cat_features if c in df.columns]\n",
    "\n",
    "le = LabelEncoder()\n",
    "for c in cat_features:\n",
    "    df[c] = df[c].astype(str)\n",
    "    df[c] = df[c].replace(\"nan\",\"NaN\")\n",
    "    df[c] = le.fit_transform(df[c])\n",
    "\n",
    "target_col = \"target\"\n",
    "\n",
    "# Mean-impute a random subset of numeric columns, or replicate exactly what you did in 5.3\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if (col not in cat_features) and (col != target_col)]\n",
    "\n",
    "num_cols_sample = random.sample(numeric_cols, min(100, len(numeric_cols)))\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[num_cols_sample] = imputer.fit_transform(df[num_cols_sample])\n",
    "\n",
    "print(\"Finished BFSI cleaning. Ready to define subset & do train/test.\")\n",
    "print(\"Current shape:\", df.shape)\n",
    "\n",
    "##############################################\n",
    "# 2) Restrict to the columns OptBinning selected\n",
    "##############################################\n",
    "# For example, suppose your final binning process (in 5.4) ended up choosing 19 columns:\n",
    "# (This list is from a hypothetical result. Replace with your real columns if they differ!)\n",
    "selected_optb_cols = [\n",
    "    \"D_112_min\", \"S_6_mean\", \"R_15_mean\", \"D_56_max\", \"D_47_mean\", \"B_6_std\",\n",
    "    \"D_51_min\", \"D_79_last\", \"B_5_max\", \"D_65_std\", \"D_81_std\", \"D_81_max\",\n",
    "    \"D_65_mean\", \"R_15_std\", \"D_127_max\", \"S_5_std\", \"R_24_std\", \"B_12_max\",\n",
    "    \"D_120_last\"\n",
    "]\n",
    "\n",
    "# We'll intersect them with df's columns, in case any are missing\n",
    "selected_optb_cols = [c for c in selected_optb_cols if c in df.columns]\n",
    "print(\"OptBinning said these columns are top:\", selected_optb_cols)\n",
    "\n",
    "# Build final subset with target\n",
    "final_cols = selected_optb_cols + [target_col]\n",
    "df = df[final_cols]\n",
    "\n",
    "print(\"Subsetting to OptBinning columns. New shape:\", df.shape)\n",
    "\n",
    "##############################################\n",
    "# 3) Split into train/test for subsequent analysis\n",
    "##############################################\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d62d4993-a76b-4c98-a337-75ca680ffdbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference data shape: (70000, 20)\n",
      "Current data shape: (30000, 20)\n"
     ]
    }
   ],
   "source": [
    "# We'll combine X_train, X_test with y_train, y_test into reference/current DataFrames.\n",
    "# This ensures that the 'target' column is included, so we can detect any target drift too.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_train = X_train.copy()\n",
    "df_train['target'] = y_train.values   # BFSI target\n",
    "\n",
    "df_test = X_test.copy()\n",
    "df_test['target'] = y_test.values     # BFSI target\n",
    "\n",
    "print(\"Reference data shape:\", df_train.shape)\n",
    "print(\"Current data shape:\", df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b75f5543-86a4-4975-8ec8-3494c13f2a71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ebf56eb-dbdd-4490-becb-ee2e9f7c0b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset, TargetDriftPreset\n",
    "\n",
    "# 1) Define column mapping if needed\n",
    "col_map = ColumnMapping(\n",
    "    target='target',\n",
    "    # If you had 'predictions' or 'scores', you could map them here:\n",
    "    # prediction='model_score'  # for instance\n",
    "    numerical_features=X_train.select_dtypes(include='number').columns.tolist(),\n",
    "    categorical_features=X_train.select_dtypes(exclude='number').columns.tolist()\n",
    ")\n",
    "\n",
    "# 2) Build a single Evidently Report with both DataDrift and TargetDrift\n",
    "combined_report = Report(\n",
    "    metrics=[\n",
    "        DataDriftPreset(),\n",
    "        TargetDriftPreset()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3) Run the report on your BFSI data\n",
    "combined_report.run(\n",
    "    reference_data=df_train,\n",
    "    current_data=df_test,\n",
    "    column_mapping=col_map\n",
    ")\n",
    "\n",
    "\n",
    "#4) save to HTML\n",
    "combined_report.save_html(\"combined_drift_report.html\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
